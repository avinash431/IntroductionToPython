{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb12ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc40a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22bebc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/avinashs/Downloads/softwares/spark-3.0.0-bin-hadoop2.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6df3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312938cb",
   "metadata": {},
   "source": [
    "### Spark Session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b12786b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/avinashs/Downloads/softwares/spark-3.0.0-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/08/13 07:26:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Anaconda python\").master(\"local[5]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb41b988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.104:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[5]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Anaconda python</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f94a2005dc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8c6b9",
   "metadata": {},
   "source": [
    "### Create Spark Context from Spark session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da1aac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3db0c3",
   "metadata": {},
   "source": [
    "### Create rdd from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_rdd = sc.parallelize(range(1,101), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02831ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ff64c",
   "metadata": {},
   "source": [
    "## Map partitions with Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f27d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elements_in_partition(index, elements):\n",
    "    for element in elements:\n",
    "        yield f\"partition: {index} has element: {element}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d715e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_rdd.mapPartitionsWithIndex(print_elements_in_partition).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda88fe5",
   "metadata": {},
   "source": [
    "### Map transfromation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049dbc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_rdd.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73f5b9",
   "metadata": {},
   "source": [
    "### Filter transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_rdd.filter(lambda x : x%2 ==0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec1704",
   "metadata": {},
   "source": [
    "### Get Number of partitions of a rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8bab01",
   "metadata": {},
   "source": [
    "### Count Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df1aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb07524",
   "metadata": {},
   "source": [
    "### flatMap transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This is a sentence1.\", \"This is sentence2.\", \"This is sentence3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_rdd = sc.parallelize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8112f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eddb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_rdd.flatMap(lambda x:x.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb302100",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/Users/avinashs/Downloads/pyspark/datasets/Baby_Names__Beginning_2007.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18140f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6847ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_baby_names_partitions_wise(index, iterator):\n",
    "    count_records = 0\n",
    "    for row in iterator:\n",
    "        count_records += 1\n",
    "    yield f\"Parition {index} has {count_records} records\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.mapPartitionsWithIndex(count_baby_names_partitions_wise).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_header(index, iterator):\n",
    "    number = 0\n",
    "    for row in iterator:\n",
    "        if index == 0 and number == 0:\n",
    "            number += 1\n",
    "            continue\n",
    "        yield row\n",
    "            \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdd = rdd.mapPartitionsWithIndex(skip_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_rdd = data_rdd.map(lambda x: (x.split(\",\")[0], int(x.split(\",\")[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_rdd.reduceByKey(lambda x, y : x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64dd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_rdd.groupByKey().mapValues(sum).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd4b51",
   "metadata": {},
   "source": [
    "### Increase the nuber of partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_rdd = year_rdd.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a81790",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87242499",
   "metadata": {},
   "source": [
    "### Decrease the number of partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_rdd.coalesce(3).getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfddd1ac",
   "metadata": {},
   "source": [
    "### Write rdd to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ac169",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_rdd.saveAsTextFile(\"years.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaedb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_repartition = rdd.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8325e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_repartition.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f78a27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd = sc.textFile(\"/Users/avinashs/PycharmProjects/introtoPython/spark/datasets/ebook.txt\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab45408",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea79b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd.flatMap(lambda x: x.split(\" \")).map(lambda x : (x,1)).reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfeaf5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/avinashs/PycharmProjects/introtoPython/spark/datasets/weather.csv\"\n",
    "file1_rdd = sc.textFile(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3b916",
   "metadata": {},
   "source": [
    "### take action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9ee9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2016-05-09,234893,34',\n",
       " '2019-09-08,234896,3',\n",
       " '2019-11-19,234895,24',\n",
       " '2017-04-04,234900,43',\n",
       " '2013-12-04,234900,47']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccb2e4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-05-09,234893,34'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a92b12f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2010, 39),\n",
       " (2011, 38),\n",
       " (2012, 40),\n",
       " (2013, 47),\n",
       " (2014, 35),\n",
       " (2015, 41),\n",
       " (2016, 36),\n",
       " (2017, 47),\n",
       " (2018, 45),\n",
       " (2019, 47)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1_rdd.map(lambda x: (x.split(\",\")[0], int(x.split(\",\")[-1]))).map(lambda x: (int(x[0].split(\"-\")[0]), x[1])).reduceByKey(lambda x, y: max(x,y)).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d233d",
   "metadata": {},
   "source": [
    "### Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b224f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30208370",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd = sc.textFile(\"/Users/avinashs/PycharmProjects/introtoPython/spark/datasets/ebook.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07751fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55673580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_blank_lines_in_partition(iterator):\n",
    "    count_blank_lines = 0\n",
    "    for row in iterator:\n",
    "        if len(row.strip()) == 0:\n",
    "            count_blank_lines += 1\n",
    "\n",
    "    accum.add(count_blank_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e84c7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd.foreachPartition(count_blank_lines_in_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "167258d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face20eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
