{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a625ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc4c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d99c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/avinashs/Downloads/softwares/spark-3.0.0-bin-hadoop2.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe49ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ff3618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/avinashs/Downloads/softwares/spark-3.0.0-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/08/14 07:07:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/14 07:07:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/14 07:07:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8abc66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([[1,2,3,4,5],[6,7,8,9,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1634a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "| _1| _2| _3| _4| _5|\n",
      "+---+---+---+---+---+\n",
      "|  1|  2|  3|  4|  5|\n",
      "|  6|  7|  8|  9| 10|\n",
      "+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a577a0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=2, _3=3, _4=4, _5=5), Row(_1=6, _2=7, _3=8, _4=9, _5=10)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c14ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([(1, \"python\"), (2, \"java\"), (3, \"c++\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fe65848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'python'), (2, 'java'), (3, 'c++')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4d6b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3f6efab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| _1|    _2|\n",
      "+---+------+\n",
      "|  1|python|\n",
      "|  2|  java|\n",
      "|  3|   c++|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42095940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method json in module pyspark.sql.readwriter:\n",
      "\n",
      "json(path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None, dropFieldIfAllNull=None, encoding=None, locale=None, pathGlobFilter=None, recursiveFileLookup=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      "    \n",
      "    `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      "    For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      "    \n",
      "    If the ``schema`` parameter is not specified, this function goes\n",
      "    through the input once to determine the input schema.\n",
      "    \n",
      "    :param path: string represents path to the JSON dataset, or a list of paths,\n",
      "                 or RDD of Strings storing JSON objects.\n",
      "    :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
      "                   a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    :param primitivesAsString: infers all primitive values as a string type. If None is set,\n",
      "                               it uses the default value, ``false``.\n",
      "    :param prefersDecimal: infers all floating-point values as a decimal type. If the values\n",
      "                           do not fit in decimal, then it infers them as doubles. If None is\n",
      "                           set, it uses the default value, ``false``.\n",
      "    :param allowComments: ignores Java/C++ style comment in JSON records. If None is set,\n",
      "                          it uses the default value, ``false``.\n",
      "    :param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,\n",
      "                                    it uses the default value, ``false``.\n",
      "    :param allowSingleQuotes: allows single quotes in addition to double quotes. If None is\n",
      "                                    set, it uses the default value, ``true``.\n",
      "    :param allowNumericLeadingZero: allows leading zeros in numbers (e.g. 00012). If None is\n",
      "                                    set, it uses the default value, ``false``.\n",
      "    :param allowBackslashEscapingAnyCharacter: allows accepting quoting of all character\n",
      "                                               using backslash quoting mechanism. If None is\n",
      "                                               set, it uses the default value, ``false``.\n",
      "    :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      "                 set, it uses the default value, ``PERMISSIVE``.\n",
      "    \n",
      "            * ``PERMISSIVE``: when it meets a corrupted record, puts the malformed string                   into a field configured by ``columnNameOfCorruptRecord``, and sets malformed                   fields to ``null``. To keep corrupt records, an user can set a string type                   field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a                   schema does not have the field, it drops corrupt records during parsing.                   When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord``                   field in an output schema.\n",
      "            *  ``DROPMALFORMED``: ignores the whole corrupted records.\n",
      "            *  ``FAILFAST``: throws an exception when it meets corrupted records.\n",
      "    \n",
      "    :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      "                                      created by ``PERMISSIVE`` mode. This overrides\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      "                                      it uses the value specified in\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``.\n",
      "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      "                       follow the formats at `datetime pattern`_.\n",
      "                       This applies to date type. If None is set, it uses the\n",
      "                       default value, ``yyyy-MM-dd``.\n",
      "    :param timestampFormat: sets the string that indicates a timestamp format.\n",
      "                            Custom date formats follow the formats at `datetime pattern`_.\n",
      "                            This applies to timestamp type. If None is set, it uses the\n",
      "                            default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      "    :param multiLine: parse one record, which may span multiple lines, per file. If None is\n",
      "                      set, it uses the default value, ``false``.\n",
      "    :param allowUnquotedControlChars: allows JSON Strings to contain unquoted control\n",
      "                                      characters (ASCII characters with value less than 32,\n",
      "                                      including tab and line feed characters) or not.\n",
      "    :param encoding: allows to forcibly set one of standard basic or extended encoding for\n",
      "                     the JSON files. For example UTF-16BE, UTF-32LE. If None is set,\n",
      "                     the encoding of input JSON will be detected automatically\n",
      "                     when the multiLine option is set to ``true``.\n",
      "    :param lineSep: defines the line separator that should be used for parsing. If None is\n",
      "                    set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
      "    :param samplingRatio: defines fraction of input JSON objects used for schema inferring.\n",
      "                          If None is set, it uses the default value, ``1.0``.\n",
      "    :param dropFieldIfAllNull: whether to ignore column of all null values or empty\n",
      "                               array/struct during schema inference. If None is set, it\n",
      "                               uses the default value, ``false``.\n",
      "    :param locale: sets a locale as language tag in IETF BCP 47 format. If None is set,\n",
      "                   it uses the default value, ``en-US``. For instance, ``locale`` is used while\n",
      "                   parsing dates and timestamps.\n",
      "    :param pathGlobFilter: an optional glob pattern to only include files with paths matching\n",
      "                           the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      "                           It does not change the behavior of `partition discovery`_.\n",
      "    :param recursiveFileLookup: recursively scan a directory for files. Using this option\n",
      "                                disables `partition discovery`_.\n",
      "    \n",
      "    .. _partition discovery:\n",
      "      https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery\n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    >>> df1 = spark.read.json('python/test_support/sql/people.json')\n",
      "    >>> df1.dtypes\n",
      "    [('age', 'bigint'), ('name', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/people.json')\n",
      "    >>> df2 = spark.read.json(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('age', 'bigint'), ('name', 'string')]\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c24d9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.json(\"/Users/avinashs/PycharmProjects/introtoPython/spark/datasets/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10b9498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dff1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_names_df = spark.read.csv(\"/Users/avinashs/PycharmProjects/introtoPython/spark/datasets/Baby_Names__Beginning_2007.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed5a337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+---+-----+\n",
      "| _c0|        _c1|        _c2|_c3|  _c4|\n",
      "+----+-----------+-----------+---+-----+\n",
      "|Year| First Name|     County|Sex|Count|\n",
      "|2013|      GAVIN|ST LAWRENCE|  M|    9|\n",
      "|2013|       LEVI|ST LAWRENCE|  M|    9|\n",
      "|2013|      LOGAN|   NEW YORK|  M|   44|\n",
      "|2013|     HUDSON|   NEW YORK|  M|   49|\n",
      "|2013|    GABRIEL|   NEW YORK|  M|   50|\n",
      "|2013|   THEODORE|   NEW YORK|  M|   51|\n",
      "|2013|      ELIZA|      KINGS|  F|   16|\n",
      "|2013|  MADELEINE|      KINGS|  F|   16|\n",
      "|2013|       ZARA|      KINGS|  F|   16|\n",
      "|2013|      DAISY|      KINGS|  F|   16|\n",
      "|2013|   JONATHAN|   NEW YORK|  M|   51|\n",
      "|2013|CHRISTOPHER|   NEW YORK|  M|   52|\n",
      "|2013|       LUKE|    SUFFOLK|  M|   49|\n",
      "|2013|    JACKSON|   NEW YORK|  M|   53|\n",
      "|2013|    JACKSON|    SUFFOLK|  M|   49|\n",
      "|2013|     JOSHUA|   NEW YORK|  M|   53|\n",
      "|2013|      AIDEN|   NEW YORK|  M|   53|\n",
      "|2013|    BRANDON|    SUFFOLK|  M|   50|\n",
      "|2013|       JUDY|      KINGS|  F|   16|\n",
      "+----+-----------+-----------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baby_names_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "304aa54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "    \n",
      "    This function will go through the input once to determine the input schema if\n",
      "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "    \n",
      "    :param path: string, or list of strings, for input path(s),\n",
      "                 or RDD of Strings storing CSV rows.\n",
      "    :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "                   or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    :param sep: sets a separator (one or more characters) for each field and value. If None is\n",
      "                set, it uses the default value, ``,``.\n",
      "    :param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
      "                     it uses the default value, ``UTF-8``.\n",
      "    :param quote: sets a single character used for escaping quoted values where the\n",
      "                  separator can be part of the value. If None is set, it uses the default\n",
      "                  value, ``\"``. If you would like to turn off quotations, you need to set an\n",
      "                  empty string.\n",
      "    :param escape: sets a single character used for escaping quotes inside an already\n",
      "                   quoted value. If None is set, it uses the default value, ``\\``.\n",
      "    :param comment: sets a single character used for skipping lines beginning with this\n",
      "                    character. By default (None), it is disabled.\n",
      "    :param header: uses the first line as names of columns. If None is set, it uses the\n",
      "                   default value, ``false``.\n",
      "    :param inferSchema: infers the input schema automatically from data. It requires one extra\n",
      "                   pass over the data. If None is set, it uses the default value, ``false``.\n",
      "    :param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n",
      "                          forcibly applied to datasource files, and headers in CSV files will be\n",
      "                          ignored. If the option is set to ``false``, the schema will be\n",
      "                          validated against all headers in CSV files or the first header in RDD\n",
      "                          if the ``header`` option is set to ``true``. Field names in the schema\n",
      "                          and column names in CSV headers are checked by their positions\n",
      "                          taking into account ``spark.sql.caseSensitive``. If None is set,\n",
      "                          ``true`` is used by default. Though the default value is ``true``,\n",
      "                          it is recommended to disable the ``enforceSchema`` option\n",
      "                          to avoid incorrect results.\n",
      "    :param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
      "                                    values being read should be skipped. If None is set, it\n",
      "                                    uses the default value, ``false``.\n",
      "    :param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
      "                                     values being read should be skipped. If None is set, it\n",
      "                                     uses the default value, ``false``.\n",
      "    :param nullValue: sets the string representation of a null value. If None is set, it uses\n",
      "                      the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
      "                      applies to all supported types including the string type.\n",
      "    :param nanValue: sets the string representation of a non-number value. If None is set, it\n",
      "                     uses the default value, ``NaN``.\n",
      "    :param positiveInf: sets the string representation of a positive infinity value. If None\n",
      "                        is set, it uses the default value, ``Inf``.\n",
      "    :param negativeInf: sets the string representation of a negative infinity value. If None\n",
      "                        is set, it uses the default value, ``Inf``.\n",
      "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      "                       follow the formats at `datetime pattern`_.\n",
      "                       This applies to date type. If None is set, it uses the\n",
      "                       default value, ``yyyy-MM-dd``.\n",
      "    :param timestampFormat: sets the string that indicates a timestamp format.\n",
      "                            Custom date formats follow the formats at `datetime pattern`_.\n",
      "                            This applies to timestamp type. If None is set, it uses the\n",
      "                            default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      "    :param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
      "                       set, it uses the default value, ``20480``.\n",
      "    :param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
      "                              value being read. If None is set, it uses the default value,\n",
      "                              ``-1`` meaning unlimited length.\n",
      "    :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
      "                                        If specified, it is ignored.\n",
      "    :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      "                 set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
      "                 parse only required columns in CSV under column pruning. Therefore, corrupt\n",
      "                 records can be different based on required set of fields. This behavior can\n",
      "                 be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
      "                 (enabled by default).\n",
      "    \n",
      "            * ``PERMISSIVE``: when it meets a corrupted record, puts the malformed string \\\n",
      "              into a field configured by ``columnNameOfCorruptRecord``, and sets malformed \\\n",
      "              fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
      "              field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
      "              schema does not have the field, it drops corrupt records during parsing. \\\n",
      "              A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
      "              When it meets a record having fewer tokens than the length of the schema, \\\n",
      "              sets ``null`` to extra fields. When the record has more tokens than the \\\n",
      "              length of the schema, it drops extra tokens.\n",
      "            * ``DROPMALFORMED``: ignores the whole corrupted records.\n",
      "            * ``FAILFAST``: throws an exception when it meets corrupted records.\n",
      "    \n",
      "    :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      "                                      created by ``PERMISSIVE`` mode. This overrides\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      "                                      it uses the value specified in\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``.\n",
      "    :param multiLine: parse records, which may span multiple lines. If None is\n",
      "                      set, it uses the default value, ``false``.\n",
      "    :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
      "                                      the quote character. If None is set, the default value is\n",
      "                                      escape character when escape and quote characters are\n",
      "                                      different, ``\\0`` otherwise.\n",
      "    :param samplingRatio: defines fraction of rows used for schema inferring.\n",
      "                          If None is set, it uses the default value, ``1.0``.\n",
      "    :param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
      "                       the default value, empty string.\n",
      "    :param locale: sets a locale as language tag in IETF BCP 47 format. If None is set,\n",
      "                   it uses the default value, ``en-US``. For instance, ``locale`` is used while\n",
      "                   parsing dates and timestamps.\n",
      "    :param lineSep: defines the line separator that should be used for parsing. If None is\n",
      "                    set, it covers all ``\\\\r``, ``\\\\r\\\\n`` and ``\\\\n``.\n",
      "                    Maximum length is 1 character.\n",
      "    :param pathGlobFilter: an optional glob pattern to only include files with paths matching\n",
      "                           the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      "                           It does not change the behavior of `partition discovery`_.\n",
      "    :param recursiveFileLookup: recursively scan a directory for files. Using this option\n",
      "                                disables `partition discovery`_.\n",
      "    \n",
      "    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      "    >>> df.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      "    >>> df2 = spark.read.csv(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    \n",
      "    .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0573af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: string, First Name: string, County: string, Sex: string, Count: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option(\"header\", True).csv(\"/Users/avinashs/PycharmProjects/introtoPython/spark/datasets/Baby_Names__Beginning_2007.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d5bc2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"/Users/avinashs/PycharmProjects/introtoPython/spark/datasets/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "357541fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: string, First Name: string, County: string, Sex: string, Count: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"csv\").option(\"header\", \"True\").load(\"/Users/avinashs/PycharmProjects/introtoPython/spark/datasets/Baby_Names__Beginning_2007.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a3c481",
   "metadata": {},
   "source": [
    "### Explicity creating dataframe with user-defined schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a40d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e09cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [StructField(\"year\", DateType(), True),\n",
    "                    StructField(\"first_name\", StringType(), True),\n",
    "                    StructField(\"county\", StringType(), True),\n",
    "                    StructField(\"gender\", StringType(), True),\n",
    "                    StructField(\"count\", IntegerType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d457130",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema= StructType(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8d40d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(year,DateType,true),StructField(first_name,StringType,true),StructField(county,StringType,true),StructField(gender,StringType,true),StructField(count,IntegerType,true)))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c561084",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_schema_df = spark.read.format(\"csv\").option(\"mode\", \"DROPMALFORMED\").schema(schema).load(\"/Users/avinashs/PycharmProjects/introtoPython/spark/datasets/Baby_Names__Beginning_2007.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41f1e362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+------+-----+\n",
      "|      year| first_name|     county|gender|count|\n",
      "+----------+-----------+-----------+------+-----+\n",
      "|2013-01-01|      GAVIN|ST LAWRENCE|     M|    9|\n",
      "|2013-01-01|       LEVI|ST LAWRENCE|     M|    9|\n",
      "|2013-01-01|      LOGAN|   NEW YORK|     M|   44|\n",
      "|2013-01-01|     HUDSON|   NEW YORK|     M|   49|\n",
      "|2013-01-01|    GABRIEL|   NEW YORK|     M|   50|\n",
      "|2013-01-01|   THEODORE|   NEW YORK|     M|   51|\n",
      "|2013-01-01|      ELIZA|      KINGS|     F|   16|\n",
      "|2013-01-01|  MADELEINE|      KINGS|     F|   16|\n",
      "|2013-01-01|       ZARA|      KINGS|     F|   16|\n",
      "|2013-01-01|      DAISY|      KINGS|     F|   16|\n",
      "|2013-01-01|   JONATHAN|   NEW YORK|     M|   51|\n",
      "|2013-01-01|CHRISTOPHER|   NEW YORK|     M|   52|\n",
      "|2013-01-01|       LUKE|    SUFFOLK|     M|   49|\n",
      "|2013-01-01|    JACKSON|   NEW YORK|     M|   53|\n",
      "|2013-01-01|    JACKSON|    SUFFOLK|     M|   49|\n",
      "|2013-01-01|     JOSHUA|   NEW YORK|     M|   53|\n",
      "|2013-01-01|      AIDEN|   NEW YORK|     M|   53|\n",
      "|2013-01-01|    BRANDON|    SUFFOLK|     M|   50|\n",
      "|2013-01-01|       JUDY|      KINGS|     F|   16|\n",
      "|2013-01-01|      MASON|ST LAWRENCE|     M|    8|\n",
      "+----------+-----------+-----------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_schema_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb15be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8a891bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, first_name: string, county: string, gender: string, count: string]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_schema_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9e3e89f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'columnNameOfCorruptRecord'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q0/_f4zx8bs08z35px0kj37zldc0000gn/T/ipykernel_26230/3521731611.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumnNameOfCorruptRecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'columnNameOfCorruptRecord'"
     ]
    }
   ],
   "source": [
    "spark.sql.columnNameOfCorruptRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe53be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
